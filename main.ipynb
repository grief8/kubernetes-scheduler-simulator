{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sequentially split the trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "nums = ['050', '100', '200', '250', '300']\n",
    "for num in nums:\n",
    "    path = '/home/fabing/projects/kubernetes-scheduler-simulator/data/csv/openb_pod_list_cpu' + num + '.csv'\n",
    "    df = pd.read_csv(path)\n",
    "    df['num_gpu'] = 0\n",
    "    df['gpu_milli'] = 0\n",
    "    df['gpu_spec'] = ''\n",
    "    running_df = df\n",
    "    # Filter the dataframe\n",
    "    # running_df = df[df['pod_phase'] == 'Running']\n",
    "    # running_df = running_df[running_df['qos'] == 'LS']\n",
    "    running_df = running_df[running_df['num_gpu'] == 0]\n",
    "    \n",
    "    tenants = [1, 2, 3, 5, 10, 15, 20]\n",
    "    for tenant in tenants:\n",
    "        # Split the dataframe into parts\n",
    "        dfs = np.array_split(running_df, tenant)\n",
    "        # Save each part to a separate CSV file\n",
    "        for i, df_part in enumerate(dfs, 1):\n",
    "            out_file = '/home/fabing/projects/kubernetes-scheduler-simulator/data/csv/fabing_cpu_{}_{}_{}.csv'.format(num, tenant, i)\n",
    "            print('data/fabing_cpu_{}_{}_{}'.format(num, tenant, i))\n",
    "            df_part.to_csv(out_file.format(i), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataframe\n",
    "running_df = df[df['pod_phase'] == 'Running']\n",
    "\n",
    "# running_df = running_df[running_df['qos'] == 'LS']\n",
    "running_df = running_df[running_df['num_gpu'] == 0]\n",
    "\n",
    "# Save to a new CSV file\n",
    "running_df.to_csv('/home/fabing/projects/kubernetes-scheduler-simulator/data/csv/fabing_running_all.csv', index=False)\n",
    "\n",
    "# Split the dataframe into 3 equal parts\n",
    "dfs = np.array_split(running_df, 3)\n",
    "\n",
    "# Save each part to a separate CSV file\n",
    "for i, df_part in enumerate(dfs, 1):\n",
    "    df_part.to_csv(f'/home/fabing/projects/kubernetes-scheduler-simulator/data/csv/fabing_running_part{i}_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output the number of used nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = '/home/fabing/projects/kubernetes-scheduler-simulator/experiments/2023_1123/fabing_cpu_300_20_13/05-BestFit/1.3/42/analysis_allo.csv'\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "df.columns = [x.split('-')[-1] for x in df.columns]\n",
    "dfd = df.to_dict(orient=\"list\")\n",
    "print(max(df['used_nodes']))\n",
    "total_gpu_num = df.total_gpus.values[0]\n",
    "df['arrive_ratio'] = df.arrived_gpu_milli / total_gpu_num / 10\n",
    "df['arrive_ratio'] = df['arrive_ratio'].apply(lambda x: round(x, 0))\n",
    "df['alloc_ratio'] = df.used_gpu_milli / total_gpu_num / 10\n",
    "df['alloc_ratio'] = df['alloc_ratio'].apply(lambda x: round(x, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further processing of analysis results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "out_file = '/home/fabing/projects/kubernetes-scheduler-simulator/experiments/analysis/analysis_results/analysis_allo_discrete_sum_use_nodes_uniform.csv'\n",
    "# Load the dataframe from the CSV file\n",
    "# df = pd.read_csv('/home/fabing/projects/kubernetes-scheduler-simulator/experiments/analysis/analysis_results/analysis_allo_discrete.csv')\n",
    "df = pd.read_csv('/home/fabing/projects/kubernetes-scheduler-simulator/experiments/analysis/analysis_results/analysis_allo_discrete.csv')\n",
    "df.drop(columns=['sc_policy', 'tune', 'seed'], inplace=True)\n",
    "\n",
    "# Group by 'workload' and calculate the mean of the other columns\n",
    "df = df.groupby('workload').mean()\n",
    "# Reset the index\n",
    "df = df.reset_index()\n",
    "# Extract the prefix of the 'workload' column\n",
    "df['workload_prefix'] = df['workload'].str.rsplit('_', n=1).str[0]\n",
    "\n",
    "# Calculate the average 'use_nodes' for each 'workload_prefix'\n",
    "average_use_nodes = df.groupby('workload_prefix')['use_nodes'].sum()\n",
    "average_use_nodes.to_csv(out_file)\n",
    "df = pd.read_csv(out_file)\n",
    "# Split the 'workload_prefix' into four parts\n",
    "df[['prefix', 'cpu', 'first_number', 'second_number']] = df['workload_prefix'].str.split('_', n=3, expand=True)\n",
    "\n",
    "# Convert the 'first_number' and 'second_number' columns to numeric types\n",
    "df[['first_number', 'second_number']] = df[['first_number', 'second_number']].apply(pd.to_numeric)\n",
    "\n",
    "# Group by 'first_number' and calculate the mean of 'use_nodes'\n",
    "grouped_df = df.groupby('first_number')['use_nodes'].mean()\n",
    "\n",
    "# Sort by 'second_number'\n",
    "sorted_df = df.sort_values('first_number')\n",
    "\n",
    "# Draw bar charts\n",
    "for i in sorted_df['first_number'].unique():\n",
    "    # Filter the dataframe\n",
    "    filtered_df = sorted_df[sorted_df['first_number'] == i]\n",
    "    # Sort by 'second_number'\n",
    "    filtered_df = filtered_df.sort_values('second_number')\n",
    "    # Draw a bar chart\n",
    "    filtered_df.plot.bar(x='second_number', y='use_nodes', rot=0, title=f'trace={i}', xlabel='Number of Tetants', ylabel='use_nodes')\n",
    "    plt.show()\n",
    "\n",
    "# Save to a new CSV file\n",
    "# print(sorted_df)\n",
    "# sorted_df.to_csv(out_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter the node config according to 'memory' and 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataframe from the CSV file\n",
    "path = '/home/fabing/projects/kubernetes-scheduler-simulator/data/csv/openb_node_list_all_node.csv'\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# Remove columns ['sn', 'gpu', 'model']\n",
    "df = df.drop(columns=['sn', 'gpu', 'model'])\n",
    "\n",
    "# Remove duplicate rows\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Save to a new CSV file\n",
    "df.to_csv('/home/fabing/projects/kubernetes-scheduler-simulator/simplify_node_list_all_node.csv', index=False)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uniformly sample the trace and generate the corresponding table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "nums = ['050', '100', '200', '250', '300']\n",
    "for num in nums:\n",
    "    path = '/home/fabing/projects/kubernetes-scheduler-simulator/data/csv/openb_pod_list_cpu' + num + '.csv'\n",
    "    df = pd.read_csv(path)\n",
    "    df['num_gpu'] = 0\n",
    "    df['gpu_milli'] = 0\n",
    "    df['gpu_spec'] = ''\n",
    "    running_df = df\n",
    "    # Filter the dataframe\n",
    "    # running_df = df[df['pod_phase'] == 'Running']\n",
    "    # running_df = running_df[running_df['qos'] == 'LS']\n",
    "    # running_df = running_df[running_df['num_gpu'] == 0]\n",
    "    \n",
    "    tenants = [1, 2, 3, 5, 10, 15, 20]\n",
    "    for tenant in tenants:\n",
    "        # Split the dataframe into parts\n",
    "        dfs = [df.iloc[i::tenant].reset_index(drop=True) for i in range(tenant)]\n",
    "        # Save each part to a separate CSV file\n",
    "        for i, df_part in enumerate(dfs, 1):\n",
    "            out_file = '/home/fabing/projects/kubernetes-scheduler-simulator/data/csv/fabing_uniform_{}_{}_{}.csv'.format(num, tenant, i)\n",
    "            print('data/fabing_uniform_{}_{}_{}'.format(num, tenant, i))\n",
    "            df_part.to_csv(out_file.format(i), index=False)\n",
    "            # print(df_part.head(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the price to the node list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataframe from the CSV file\n",
    "path = '/home/fabing/projects/kubernetes-scheduler-simulator/data/csv/openb_node_list_all_node.csv'\n",
    "node_df = pd.read_csv(path)\n",
    "\n",
    "# Load the price dataframe from the CSV file\n",
    "path = '/home/fabing/projects/kubernetes-scheduler-simulator/node_list_price.csv'\n",
    "price_df = pd.read_csv(path)\n",
    "# Remove commas and convert to float\n",
    "price_df['Aliyun cost (yuan/month)'] = price_df['Aliyun cost (yuan/month)'].replace(',', '', regex=True).astype(float)\n",
    "\n",
    "result_df = pd.merge(node_df, price_df, on=['cpu_milli', 'memory_mib'], how='left')\n",
    "result_df.to_csv('/home/fabing/projects/kubernetes-scheduler-simulator/node_list_all_node_price.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = '/home/fabing/projects/kubernetes-scheduler-simulator/experiments/2023_cpu/fabing_cpu_050_1_1/05-BestFit/1.3/42/analysis_price.csv'\n",
    "price_path = '/home/fabing/projects/kubernetes-scheduler-simulator/node_list_all_node_price.csv'\n",
    "\n",
    "price_df = pd.read_csv(price_path)\n",
    "total_cost = 0\n",
    "with open(path, 'r') as f:\n",
    "    line = f.readline()\n",
    "    line = line.replace(r'\\n', '')\n",
    "    line = line.split(r',')\n",
    "    for sn in line:\n",
    "        sn = sn.strip()\n",
    "        try:\n",
    "            aliyun_cost = price_df.loc[price_df['sn'] == sn, 'Aliyun cost (yuan/month)']\n",
    "            total_cost += aliyun_cost.values[0]\n",
    "        except IndexError:\n",
    "            print(f\"No match found for sn: {sn}\")\n",
    "    print(total_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the node list and the table, and calulate the price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataframe from the CSV file\n",
    "path = '/home/fabing/projects/kubernetes-scheduler-simulator/experiments/analysis/analysis_results/analysis_price_discrete.csv'\n",
    "out_file = '/home/fabing/projects/kubernetes-scheduler-simulator/experiments/analysis/analysis_results/uniform_analysis_price_discrete_sum.csv'\n",
    "df = pd.read_csv(path)\n",
    "# Extract the prefix of the 'workload' column\n",
    "df['workload_prefix'] = df['workload'].str.rsplit('_', n=1).str[0]\n",
    "\n",
    "# Calculate the average 'use_nodes' for each 'workload_prefix'\n",
    "df = df.groupby('workload_prefix').sum()\n",
    "df.drop(columns=['workload', 'sc_policy', 'tune'], inplace=True)\n",
    "df.to_csv(out_file)\n",
    "df = pd.read_csv(out_file)\n",
    "# Split the 'workload_prefix' into four parts\n",
    "df[['prefix', 'type', 'traces', 'tenants']] = df['workload_prefix'].str.split('_', expand=True)\n",
    "# Convert the 'traces' and 'tenants' columns to numeric types\n",
    "df[['traces', 'tenants']] = df[['traces', 'tenants']].apply(pd.to_numeric)\n",
    "# df['cpu'] = df['cpu'].astype(float)\n",
    "# df['mem'] = df['mem'].astype(float)\n",
    "df['cpu'] = df['cpu'] / 1000\n",
    "df['mem'] = df['mem'] / 1024\n",
    "# Group by 'first_number' and calculate the mean of 'use_nodes'\n",
    "# grouped_df = df.groupby('first_number')['cost'].mean()\n",
    "\n",
    "# Sort by 'second_number'\n",
    "sorted_df = df.sort_values('traces')\n",
    "# Define color list\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
    "\n",
    "# Define bar width\n",
    "bar_width = 0.15\n",
    "\n",
    "# Create a new figure for 'cpu'\n",
    "plt.figure()\n",
    "# Loop over unique traces\n",
    "for idx, i in enumerate(sorted_df['traces'].unique()):\n",
    "    # Filter the dataframe\n",
    "    filtered_df = sorted_df[sorted_df['traces'] == i]\n",
    "    # Sort by 'tenants'\n",
    "    filtered_df = filtered_df.sort_values('tenants')\n",
    "    # Draw a bar chart for 'cpu' with offset\n",
    "    plt.bar(filtered_df['tenants'] + idx * bar_width, filtered_df['cpu'], width=bar_width, color=colors[idx % len(colors)], label=f'cpu_trace={i}')\n",
    "    # Add scatter plot and line\n",
    "    # plt.scatter(filtered_df['tenants'] + idx * bar_width, filtered_df['cpu'], color=colors[idx % len(colors)])\n",
    "# Set xticks\n",
    "plt.xticks(sorted_df['tenants'].unique())\n",
    "# Add a legend\n",
    "plt.legend(loc='upper left')\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Create a new figure for 'mem'\n",
    "plt.figure()\n",
    "# Loop over unique traces\n",
    "for idx, i in enumerate(sorted_df['traces'].unique()):\n",
    "    # Filter the dataframe\n",
    "    filtered_df = sorted_df[sorted_df['traces'] == i]\n",
    "    # Sort by 'tenants'\n",
    "    filtered_df = filtered_df.sort_values('tenants')\n",
    "    # Draw a bar chart for 'mem' with offset\n",
    "    plt.bar(filtered_df['tenants'] + idx * bar_width, filtered_df['mem'], width=bar_width, color=colors[idx % len(colors)], label=f'mem_trace={i}')\n",
    "# Set xticks\n",
    "plt.xticks(sorted_df['tenants'].unique())\n",
    "# Add a legend\n",
    "plt.legend(loc='upper left')\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Create a new figure for 'cost'\n",
    "plt.figure()\n",
    "# Loop over unique traces\n",
    "for idx, i in enumerate(sorted_df['traces'].unique()):\n",
    "    # Filter the dataframe\n",
    "    filtered_df = sorted_df[sorted_df['traces'] == i]\n",
    "    # Sort by 'tenants'\n",
    "    filtered_df = filtered_df.sort_values('tenants')\n",
    "    # Draw a bar chart for 'cost' with offset\n",
    "    plt.bar(filtered_df['tenants'] + idx * bar_width, filtered_df['cost'], width=bar_width, color=colors[idx % len(colors)], label=f'cost_trace={i}')\n",
    "# Set xticks\n",
    "plt.xticks(sorted_df['tenants'].unique())\n",
    "# Add a legend\n",
    "plt.legend(loc='upper left')\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# save to a new CSV file\n",
    "sorted_df.to_csv(out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw bar charts\n",
    "for i in sorted_df['traces'].unique():\n",
    "    # Filter the dataframe\n",
    "    filtered_df = sorted_df[sorted_df['traces'] == i]\n",
    "    # Sort by 'second_number'\n",
    "    filtered_df = filtered_df.sort_values('tenants')\n",
    "    # Draw a bar chart for 'cpu'\n",
    "    ax = filtered_df.plot.bar(x='tenants', y='cpu', rot=0, title=f'trace={i}', xlabel='Number of Tenants', ylabel='Usage of CPU')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "    # Draw a bar chart for 'mem'\n",
    "    ax = filtered_df.plot.bar(x='tenants', y='mem', rot=0, title=f'trace={i}', xlabel='Number of Tenants', ylabel='Usage of Memory')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "    # Draw a bar chart for 'cost'\n",
    "    ax = filtered_df.plot.bar(x='tenants', y='cost', rot=0, title=f'trace={i}', xlabel='Number of Tenants', ylabel='Cost')\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/fabing/projects/kubernetes-scheduler-simulator/data/csv/openb_pod_list_cpu050.csv'\n",
    "data = pd.read_csv(path)\n",
    "\n",
    "# average delete time - average create time\n",
    "duration = data['deletion_time'].mean() - data['creation_time'].mean()\n",
    "print(duration/60)\n",
    "print(len(data)/(duration/60))\n",
    "# 40 minutes for openstack 2700 events and 2100 RPC\n",
    "# 444 minites for 10095 pods\n",
    "2100/40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resource usage of each tenant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save to /home/fabing/projects/kubernetes-scheduler-simulator/experiments/analysis/analysis_results/uniform_analysis_price_discrete_sum.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataframe from the CSV file\n",
    "path = '/home/fabing/projects/kubernetes-scheduler-simulator/experiments/analysis/analysis_results/analysis_price_discrete.csv'\n",
    "out_file = '/home/fabing/projects/kubernetes-scheduler-simulator/experiments/analysis/analysis_results/uniform_analysis_price_discrete_sum.csv'\n",
    "df = pd.read_csv(path)\n",
    "# Extract the prefix of the 'workload' column\n",
    "df['workload_prefix'] = df['workload'].str.rsplit('_', n=1).str[0]\n",
    "\n",
    "# Calculate the average 'use_nodes' for each 'workload_prefix'\n",
    "df = df.groupby('workload_prefix').sum()\n",
    "df.drop(columns=['workload', 'sc_policy', 'tune'], inplace=True)\n",
    "df.to_csv(out_file)\n",
    "df = pd.read_csv(out_file)\n",
    "# Split the 'workload_prefix' into four parts\n",
    "df[['prefix', 'type', 'traces', 'tenants']] = df['workload_prefix'].str.split('_', expand=True)\n",
    "# Convert the 'traces' and 'tenants' columns to numeric types\n",
    "df[['traces', 'tenants']] = df[['traces', 'tenants']].apply(pd.to_numeric)\n",
    "\n",
    "# Sort by 'second_number'\n",
    "sorted_df = df.sort_values('traces')\n",
    "sorted_df.to_csv(out_file)\n",
    "print('save to {}'.format(out_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "050\n",
      "72761612\n",
      "268502666\n",
      "100\n",
      "80292312\n",
      "289071014\n",
      "200\n",
      "97253012\n",
      "336263486\n",
      "250\n",
      "107460312\n",
      "365787661\n",
      "300\n",
      "119444912\n",
      "397746134\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "traces = ['050', '100', '200', '250', '300']\n",
    "for trace in traces:\n",
    "    path = '/home/fabing/projects/kubernetes-scheduler-simulator/data/csv/openb_pod_list_cpu{}.csv'.format(trace)\n",
    "    data = pd.read_csv(path)\n",
    "\n",
    "    # print sum of column cpu_milli and memory_mib\n",
    "    print(trace)\n",
    "    print(data['cpu_milli'].sum())\n",
    "    print(data['memory_mib'].sum())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
