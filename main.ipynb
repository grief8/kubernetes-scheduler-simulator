{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sequentially split the trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "nums = ['050', '100', '200', '250', '300']\n",
    "for num in nums:\n",
    "    path = '/home/fabing/projects/kubernetes-scheduler-simulator/data/csv/openb_pod_list_cpu' + num + '.csv'\n",
    "    df = pd.read_csv(path)\n",
    "    # Filter the dataframe\n",
    "    running_df = df[df['pod_phase'] == 'Running']\n",
    "    # running_df = running_df[running_df['qos'] == 'LS']\n",
    "    running_df = running_df[running_df['num_gpu'] == 0]\n",
    "    \n",
    "    tenants = [1, 2, 3, 5, 10, 15, 20]\n",
    "    for tenant in tenants:\n",
    "        # Split the dataframe into parts\n",
    "        dfs = np.array_split(running_df, tenant)\n",
    "        # Save each part to a separate CSV file\n",
    "        for i, df_part in enumerate(dfs, 1):\n",
    "            out_file = '/home/fabing/projects/kubernetes-scheduler-simulator/data/csv/fabing_cpu_{}_{}_{}.csv'.format(num, tenant, i)\n",
    "            print('data/fabing_cpu_{}_{}_{}'.format(num, tenant, i))\n",
    "            df_part.to_csv(out_file.format(i), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataframe\n",
    "running_df = df[df['pod_phase'] == 'Running']\n",
    "\n",
    "# running_df = running_df[running_df['qos'] == 'LS']\n",
    "running_df = running_df[running_df['num_gpu'] == 0]\n",
    "\n",
    "# Save to a new CSV file\n",
    "running_df.to_csv('/home/fabing/projects/kubernetes-scheduler-simulator/data/csv/fabing_running_all.csv', index=False)\n",
    "\n",
    "# Split the dataframe into 3 equal parts\n",
    "dfs = np.array_split(running_df, 3)\n",
    "\n",
    "# Save each part to a separate CSV file\n",
    "for i, df_part in enumerate(dfs, 1):\n",
    "    df_part.to_csv(f'/home/fabing/projects/kubernetes-scheduler-simulator/data/csv/fabing_running_part{i}_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output the number of used nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "path = '/home/fabing/projects/kubernetes-scheduler-simulator/experiments/2023_1123/fabing_cpu_300_20_13/05-BestFit/1.3/42/analysis_allo.csv'\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "df.columns = [x.split('-')[-1] for x in df.columns]\n",
    "dfd = df.to_dict(orient=\"list\")\n",
    "print(max(df['used_nodes']))\n",
    "total_gpu_num = df.total_gpus.values[0]\n",
    "df['arrive_ratio'] = df.arrived_gpu_milli / total_gpu_num / 10\n",
    "df['arrive_ratio'] = df['arrive_ratio'].apply(lambda x: round(x, 0))\n",
    "df['alloc_ratio'] = df.used_gpu_milli / total_gpu_num / 10\n",
    "df['alloc_ratio'] = df['alloc_ratio'].apply(lambda x: round(x, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further processing of analysis results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataframe from the CSV file\n",
    "df = pd.read_csv('/home/fabing/projects/kubernetes-scheduler-simulator/experiments/analysis/analysis_results/analysis_allo_discrete.csv')\n",
    "df.drop(columns=['sc_policy', 'tune', 'seed'], inplace=True)\n",
    "\n",
    "# Group by 'workload' and calculate the mean of the other columns\n",
    "df = df.groupby('workload').mean()\n",
    "# Reset the index\n",
    "df = df.reset_index()\n",
    "# Extract the prefix of the 'workload' column\n",
    "df['workload_prefix'] = df['workload'].str.rsplit('_', n=1).str[0]\n",
    "\n",
    "# Calculate the average 'use_nodes' for each 'workload_prefix'\n",
    "average_use_nodes = df.groupby('workload_prefix')['use_nodes'].sum()\n",
    "average_use_nodes.to_csv('/home/fabing/projects/kubernetes-scheduler-simulator/experiments/analysis/analysis_results/analysis_allo_discrete_sum_use_nodes_uniform.csv')\n",
    "df = pd.read_csv('/home/fabing/projects/kubernetes-scheduler-simulator/experiments/analysis/analysis_results/analysis_allo_discrete_sum_use_nodes_uniform.csv')\n",
    "# Split the 'workload_prefix' into four parts\n",
    "df[['prefix', 'cpu', 'first_number', 'second_number']] = df['workload_prefix'].str.split('_', n=3, expand=True)\n",
    "\n",
    "# Convert the 'first_number' and 'second_number' columns to numeric types\n",
    "df[['first_number', 'second_number']] = df[['first_number', 'second_number']].apply(pd.to_numeric)\n",
    "\n",
    "# Group by 'first_number' and calculate the mean of 'use_nodes'\n",
    "grouped_df = df.groupby('first_number')['use_nodes'].mean()\n",
    "\n",
    "# Sort by 'second_number'\n",
    "sorted_df = df.sort_values('first_number')\n",
    "\n",
    "print(sorted_df)\n",
    "sorted_df.to_csv('/home/fabing/projects/kubernetes-scheduler-simulator/experiments/analysis/analysis_results/analysis_allo_discrete_sum_use_nodes_uniform.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter the node config according to 'memory' and 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataframe from the CSV file\n",
    "path = '/home/fabing/projects/kubernetes-scheduler-simulator/data/csv/openb_node_list_all_node.csv'\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# Remove columns ['sn', 'gpu', 'model']\n",
    "df = df.drop(columns=['sn', 'gpu', 'model'])\n",
    "\n",
    "# Remove duplicate rows\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Save to a new CSV file\n",
    "df.to_csv('/home/fabing/projects/kubernetes-scheduler-simulator/simplify_node_list_all_node.csv', index=False)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uniformly sample the trace and generate the corresponding table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "nums = ['050', '100', '200', '250', '300']\n",
    "for num in nums:\n",
    "    path = '/home/fabing/projects/kubernetes-scheduler-simulator/data/csv/openb_pod_list_cpu' + num + '.csv'\n",
    "    df = pd.read_csv(path)\n",
    "    # Filter the dataframe\n",
    "    running_df = df[df['pod_phase'] == 'Running']\n",
    "    # running_df = running_df[running_df['qos'] == 'LS']\n",
    "    running_df = running_df[running_df['num_gpu'] == 0]\n",
    "    \n",
    "    tenants = [1, 2, 3, 5, 10, 15, 20]\n",
    "    for tenant in tenants:\n",
    "        # Split the dataframe into parts\n",
    "        dfs = [df.iloc[i::tenant].reset_index(drop=True) for i in range(tenant)]\n",
    "        # Save each part to a separate CSV file\n",
    "        for i, df_part in enumerate(dfs, 1):\n",
    "            out_file = '/home/fabing/projects/kubernetes-scheduler-simulator/data/csv/fabing_uniform_{}_{}_{}.csv'.format(num, tenant, i)\n",
    "            print('data/fabing_uniform_{}_{}_{}'.format(num, tenant, i))\n",
    "            df_part.to_csv(out_file.format(i), index=False)\n",
    "            # print(df_part.head(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the price to the node list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataframe from the CSV file\n",
    "path = '/home/fabing/projects/kubernetes-scheduler-simulator/data/csv/openb_node_list_all_node.csv'\n",
    "node_df = pd.read_csv(path)\n",
    "\n",
    "# Load the price dataframe from the CSV file\n",
    "path = '/home/fabing/projects/kubernetes-scheduler-simulator/node_list_price.csv'\n",
    "price_df = pd.read_csv(path)\n",
    "# Remove commas and convert to float\n",
    "price_df['Aliyun cost (yuan/month)'] = price_df['Aliyun cost (yuan/month)'].replace(',', '', regex=True).astype(float)\n",
    "\n",
    "result_df = pd.merge(node_df, price_df, on=['cpu_milli', 'memory_mib'], how='left')\n",
    "result_df.to_csv('/home/fabing/projects/kubernetes-scheduler-simulator/node_list_all_node_price.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
